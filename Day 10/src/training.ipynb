{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interpolation methods\n",
    "\n",
    "    Bilinear : takes the weighted average of four nearest pixels in the original image \n",
    "               to compute the value of a new pixel in the scaled image\n",
    "\n",
    "    Nearest-neighbor: picks the pixel value of the nearest pixel in the input image and\n",
    "                      uses it for the output pixel. It is the fastest but results in\n",
    "                      blocky or pixelated output.\n",
    "\n",
    "    Bicubic interpolation: This method is more computationally expensive than bilinear \n",
    "                           interpolation and is known to produce smoother output. \n",
    "                           It takes a weighted average of 16 surrounding pixels \n",
    "                           in a 4x4 pixel neighborhood of the input image.\n",
    "\n",
    "    Lanczos interpolation: This method is a more advanced version of bicubic interpolation \n",
    "                           that is known to produce higher quality output\n",
    "                           but is even more computationally expensive.\n",
    "    \n",
    "    interpolation\n",
    "        0 -> Nearest-neighbor interpolation\n",
    "        1 -> Bilinear interpolation\n",
    "        2 -> Bicubic interpolation\n",
    "        3 -> Lanczos interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA'\n",
    "no_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/NO-GLAUCOMA'\n",
    "severe_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA'\n",
    "target_size = 224\n",
    "interpolation_type = 2\n",
    "normalization_mean = (0.5, 0.5, 0.5)\n",
    "normalization_std = (0.5, 0.5, 0.5)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = transforms.Resize((target_size, target_size), interpolation=interpolation_type)\n",
    "tensor_transform = transforms.ToTensor()\n",
    "normalize_transform = transforms.Normalize(normalization_mean, normalization_std)\n",
    "transform = transforms.Compose([\n",
    "            resize_transform,\n",
    "            tensor_transform,\n",
    "            normalize_transform\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA/N116231_20170615_125121_Color_R_001.JPG')\n",
    "image_tensor = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFolderLoader(Dataset):\n",
    "    def __init__(self, image_dir, label, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(object):\n",
    "    def __init__(self, target_size_height, target_size_width):\n",
    "        self.moderate_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/MODERATE-GLAUCOMA/'\n",
    "        self.no_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/NO-GLAUCOMA/'\n",
    "        self.severe_glucoma_folder = '/home/dileepkumar/Z_ISL/AI-Workshop-Materials/Day 10/data/SEVERE-GLAUCOMA/'\n",
    "        self.target_size_height = target_size_height\n",
    "        self.target_size_width = target_size_width\n",
    "        self.interpolation_type = 2\n",
    "        self.normalization_mean = (0.5, 0.5, 0.5)\n",
    "        self.normalization_std = (0.5, 0.5, 0.5)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.target_size_height, self.target_size_width), interpolation = self.interpolation_type),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.normalization_mean, self.normalization_std)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def load_data(self, label, folder):\n",
    "        data = DatasetFolderLoader(folder, label, self.transform)\n",
    "        train_size = int(0.8 * len(data))\n",
    "        test_size = len(data) - train_size\n",
    "        train_data, test_data = random_split(data, [train_size, test_size])\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def load_customdataset(self):\n",
    "        \"\"\" 0: moderate_glucoma, 1: no_glucoma, 2: severe_glucoma\"\"\"\n",
    "        moderate_glucoma_train, moderate_glucoma_test = self.load_data(0, self.moderate_glucoma_folder)\n",
    "        no_glucoma_train, no_glucoma_test = self.load_data(1, self.no_glucoma_folder)\n",
    "        severe_glucoma_train, severe_glucoma_test = self.load_data(2, self.severe_glucoma_folder)\n",
    "        all_training_data = torch.utils.data.ConcatDataset([moderate_glucoma_train, no_glucoma_train, severe_glucoma_train])\n",
    "        all_testing_data = torch.utils.data.ConcatDataset([moderate_glucoma_test, no_glucoma_test, severe_glucoma_test])\n",
    "        return all_training_data, all_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train, all_test = MyCustomDataset(224, 224).load_customdataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train), len(all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCustomCNN, self).__init__()\n",
    "        self.input_depth = 3\n",
    "        self.output_classes = 3\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(10, 6, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(6 * 112 * 112, 120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120, 10),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(10, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.size())\n",
    "        \"\"\"\n",
    "            After the first convolutional layer: \n",
    "                output size = (input size - kernel size + 2 * padding) / stride + 1 \n",
    "                            = (224 - 5 + 2) / 1 + 1 = 222\n",
    "            After the second convolutional layer: \n",
    "                output size = (input size - kernel size + 2 * padding) / stride + 1 \n",
    "                            = (222 - 5 + 2) / 1 + 1 = 220\n",
    "            After the maxpool layer: \n",
    "                output size = (input size - kernel size) / stride + 1 \n",
    "                            = (220 - 2) / 2 + 1 = 110\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 6 * 112 * 112)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data, all_test_data = MyCustomDataset(224, 224).load_customdataset()\n",
    "\n",
    "train_loader = DataLoader(all_train_data, batch_size=30, shuffle=True)\n",
    "# Create the custom CNN model\n",
    "model = MyCustomCNN().to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [1/10], Loss: 1.1322\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [2/10], Loss: 1.1100\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [3/10], Loss: 1.1006\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [4/10], Loss: 1.0986\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [5/10], Loss: 1.0976\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [6/10], Loss: 1.0945\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [7/10], Loss: 1.0922\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [8/10], Loss: 1.0898\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [9/10], Loss: 1.0794\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "torch.Size([30, 6, 112, 112])\n",
      "Epoch [10/10], Loss: 1.0715\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Define the path to save the model\n",
    "model_path = 'my_model.pt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

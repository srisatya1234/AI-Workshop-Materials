{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# taking all rpm samples together - each rpm files has all 4 classes - normal, BCI, BCO,BCR\n",
    "# all samples are mixed and 25 : 75 are taken as\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# choose which frequency to test\n",
    "freq = 48000                        #12000,  48000\n",
    "# choose which crack size to test\n",
    "cracksize = 14                      # 7, 14, 21\n",
    "# choose which rpm to be used for training , the remaining 3 will be used for testing\n",
    "# 1 - rpm_1797, 2 - rpm_1772, 3 - rpm_1748, 4 - rpm_1724\n",
    "whichonein4 = 1                     # 1, 2, 3, 4\n",
    "\n",
    "class dimensions:\n",
    "    # folder to input data\n",
    "    if freq == 12000:\n",
    "        folder = 'D:\\\\Lab Project Files\\\\MY1DCNN\\\\Data\\\\raw\\\\12KHz\\\\'\n",
    "    elif freq == 48000:\n",
    "        folder = 'D:\\\\Lab Project Files\\\\MY1DCNN\\\\Data\\\\raw\\\\48KHz\\\\'\n",
    "\n",
    "    num_len = 4096\n",
    "    # loading the train and test files based on the choice\n",
    "    if cracksize == 7:\n",
    "        if whichonein4 == 1:\n",
    "            files = ['rpm_1797_7mm.txt', 'rpm_1772_7mm.txt', 'rpm_1748_7mm.txt', 'rpm_1724_7mm.txt']\n",
    "        elif whichonein4 == 2:\n",
    "            files = ['rpm_1772_7mm.txt', 'rpm_1748_7mm.txt', 'rpm_1724_7mm.txt', 'rpm_1797_7mm.txt']\n",
    "        elif whichonein4 == 3:\n",
    "            files = ['rpm_1748_7mm.txt', 'rpm_1724_7mm.txt', 'rpm_1797_7mm.txt', 'rpm_1772_7mm.txt']\n",
    "        elif whichonein4 == 4:\n",
    "            files = ['rpm_1724_7mm.txt', 'rpm_1797_7mm.txt', 'rpm_1772_7mm.txt', 'rpm_1748_7mm.txt']\n",
    "\n",
    "    elif cracksize == 14:\n",
    "        if whichonein4 == 1:\n",
    "            files = ['rpm_1797_14mm.txt', 'rpm_1772_14mm.txt', 'rpm_1748_14mm.txt', 'rpm_1724_14mm.txt']\n",
    "        elif whichonein4 == 2:\n",
    "            files = ['rpm_1772_14mm.txt', 'rpm_1748_14mm.txt', 'rpm_1724_14mm.txt', 'rpm_1797_14mm.txt']\n",
    "        elif whichonein4 == 3:\n",
    "            files = ['rpm_1748_14mm.txt', 'rpm_1724_14mm.txt', 'rpm_1797_14mm.txt', 'rpm_1772_14mm.txt']\n",
    "        elif whichonein4 == 4:\n",
    "            files = ['rpm_1724_14mm.txt', 'rpm_1797_14mm.txt', 'rpm_1772_14mm.txt', 'rpm_1748_14mm.txt']\n",
    "\n",
    "    elif cracksize == 21:\n",
    "        if whichonein4 == 1:\n",
    "            files = ['rpm_1797_21mm.txt', 'rpm_1772_21mm.txt', 'rpm_1748_21mm.txt', 'rpm_1724_21mm.txt']\n",
    "        elif whichonein4 == 2:\n",
    "            files = ['rpm_1772_21mm.txt', 'rpm_1748_21mm.txt', 'rpm_1724_21mm.txt', 'rpm_1797_21mm.txt']\n",
    "        elif whichonein4 == 3:\n",
    "            files = ['rpm_1748_21mm.txt', 'rpm_1724_21mm.txt', 'rpm_1797_21mm.txt', 'rpm_1772_21mm.txt']\n",
    "        elif whichonein4 == 4:\n",
    "            files = ['rpm_1724_21mm.txt', 'rpm_1797_21mm.txt', 'rpm_1772_21mm.txt', 'rpm_1748_21mm.txt']\n",
    "\n",
    "    # selecting the parameters of the deep CNN\n",
    "    # input dimension of the CNN\n",
    "    input_width, input_height, input_depth   = 1, num_len, 1\n",
    "    # kernel sizes of first layer\n",
    "    conv1_filterwidth, conv1_filterheight, conv1_filters = 1, 5, 4 # 16 filters of 5x5\n",
    "    # kernel sizes of the second layer\n",
    "    conv2_filterwidth, conv2_filterheight, conv2_filters = 1, 5, 4 # 16 filters of 5x5\n",
    "    # pooling layer dimensions\n",
    "    pool_filterwidth, pool_filterheight = 1, 2\n",
    "    # strides in conv layer\n",
    "    conv_stridewidth, conv_strideheight = 1, 1  # strides in convolution layer\n",
    "    # pool strides in pooling layer\n",
    "    pool_stridewidth, pool_strideheight = 1, 2  # strides in pooling layer / spatial extent\n",
    "    # number of nodes in fully connected node\n",
    "    # no particular choice - optimally choosen\n",
    "    fc_nodes   = 64\n",
    "    # number of output classes\n",
    "    No_Classes = 4\n",
    "    # batch size for each iterations\n",
    "    BATCH_SIZE = 128\n",
    "    # dropuout 50% \n",
    "    dropoutrate_train = 0.5\n",
    "    # for testing no dropout\n",
    "    dropoutrate_test = 1.0\n",
    "    # choosen experimentally where the alogorthm is converging\n",
    "    no_iterations = 40\n",
    "    # k fold cross validation\n",
    "    cross_validation = 10\n",
    "    #operations\n",
    "    #paddings = 'SAME' # obtain the same size as input\n",
    "    paddings = 'VALID'\n",
    "\n",
    "    # defining the dimensions of the CONVNET layer\n",
    "    def conv_output(W1, H1, F_width, F_height, paddings, S_W, S_H):\n",
    "        if paddings == 'SAME':\n",
    "            P_width  = (F_width - 1)/2\n",
    "            P_height = (F_height- 1)/2\n",
    "        elif paddings == 'VALID':\n",
    "            P_width = 0.00\n",
    "            P_height= 0.00\n",
    "        W2 = int((W1 - F_width  + 2*P_width)/S_W + 1)\n",
    "        H2 = int((H1 - F_height + 2*P_height)/S_H + 1)\n",
    "        return W2, H2\n",
    "    # defining the dimensions of the pooling layer\n",
    "    def pool_output(W1, H1, F_width, F_height, S_W, S_H):\n",
    "        W2 = int((W1 - F_width) / S_W + 1)\n",
    "        H2 = int((H1 - F_height) / S_H + 1)\n",
    "        return W2, H2\n",
    "    # all weights in the network are intialized randomly\n",
    "    def init_weigths(shape):\n",
    "        return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "    # layer 1 initial weight\n",
    "    layer1_weights = init_weigths([conv1_filterwidth,\n",
    "                                   conv1_filterheight,\n",
    "                                   input_depth,\n",
    "                                   conv1_filters])\n",
    "    # layer 1 initial biases\n",
    "    layer1_biases  = tf.Variable(tf.zeros([conv1_filters]))\n",
    "    # layer 2 initial weights\n",
    "    layer2_weights = init_weigths([conv2_filterwidth,\n",
    "                                   conv2_filterheight,\n",
    "                                   conv1_filters,\n",
    "                                   conv2_filters])\n",
    "    # layer 2 initial biases\n",
    "    layer2_biases  = tf.Variable(tf.zeros([conv2_filters]))\n",
    "\n",
    "    # sizes of each CONVNET layer after applying conv and pool operations\n",
    "    layer1_width, layer1_height = conv_output(input_width, input_height, conv1_filterwidth, conv1_filterheight, paddings, conv_stridewidth, conv_strideheight)\n",
    "    layer1_width, layer1_height = pool_output(layer1_width, layer1_height, pool_filterwidth, pool_filterheight, pool_stridewidth, pool_strideheight)\n",
    "    layer2_width, layer2_height = conv_output(layer1_width, layer1_height, conv2_filterwidth, conv2_filterheight, paddings, conv_stridewidth, conv_strideheight)\n",
    "    layer2_width, layer2_height = pool_output(layer2_width, layer2_height, pool_filterwidth, pool_filterheight, pool_stridewidth, pool_strideheight)\n",
    "    # intializing the nodes of the fully connected layer based on the dimensions of the last CONVNet layer\n",
    "    fc_weights = tf.Variable(tf.truncated_normal([conv2_filters*layer2_width*layer2_height, fc_nodes], stddev=0.01))\n",
    "    fc_biases  = tf.Variable(tf.constant(1.0, shape=[fc_nodes]))\n",
    "    # intializing the nodes for softmax layers \n",
    "    Softmax_weights = tf.Variable(tf.truncated_normal([fc_nodes, No_Classes], stddev=0.01))\n",
    "    Softmax_biases  = tf.Variable(tf.constant(1.0, shape=[No_Classes]))\n",
    "\n",
    "# load data \n",
    "# data is preprocessed and saved in txt files\n",
    "def load_data(model):\n",
    "    folder = model.folder\n",
    "    files =  model.files\n",
    "    samples = model.input_height\n",
    "    x_datatrain = []\n",
    "    x_datatest  = []\n",
    "    y_datatrain = []\n",
    "    y_datatest  = []\n",
    "    filename = folder + files[0]\n",
    "    data = np.loadtxt(filename, unpack= True)\n",
    "    x_datatrain.extend(data[:,0:samples])\n",
    "    y_datatrain.extend(data[:,samples:])\n",
    "    for i in range(1,4):\n",
    "        filename = folder + files[i]\n",
    "        data = np.loadtxt(filename, unpack = True)\n",
    "        x_datatest.extend(data[:,0:samples])\n",
    "        y_datatest.extend(data[:,samples:])\n",
    "    return x_datatrain, x_datatest, y_datatrain, y_datatest # x - data , y - labels\n",
    "\n",
    "# reshape data in to dimensions that satisfy the CNN input nodes\n",
    "def reshapedata(data_train,data_test,model):\n",
    "    data_train = np.reshape(data_train,[-1,model.input_width, model.input_height, model.input_depth])\n",
    "    data_test  = np.reshape(data_test,[-1, model.input_width, model.input_height, model.input_depth])\n",
    "    return data_train,data_test\n",
    "\n",
    "# select the data and group in to batch for batch processing single parameter update\n",
    "def batchdata(data,label, batchsize):\n",
    "    # generate random number required to batch data\n",
    "    order_num = random.sample(range(1, len(data)), batchsize)\n",
    "    data_batch = []\n",
    "    label_batch = []\n",
    "    for i in range(len(order_num)):\n",
    "        data_batch.append(data[order_num[i-1]])\n",
    "        label_batch.append(label[order_num[i-1]])\n",
    "    return data_batch, label_batch\n",
    "\n",
    "# proposed CNN architecture model\n",
    "def mycnn(x,model,p_keep_conv):\n",
    "    CS_w, CS_h = model.conv_stridewidth, model.conv_strideheight # stride in convolution\n",
    "    PF_w, PF_h = model.pool_filterwidth, model.pool_filterheight # pooling filter size\n",
    "    PS_w, PS_h = model.pool_stridewidth, model.pool_strideheight\n",
    "\n",
    "    conv1 = tf.nn.relu(tf.add(tf.nn.conv2d(x, model.layer1_weights, strides=[1, CS_w, CS_h, 1], padding= model.paddings), model.layer1_biases))\n",
    "    layer1 = tf.nn.max_pool(conv1, ksize=[1, PF_w, PF_h, 1], strides=[1, PS_w, PS_h, 1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.relu(tf.add(tf.nn.conv2d(layer1, model.layer2_weights, strides=[1, CS_w, CS_h, 1], padding=model.paddings), model.layer2_biases))\n",
    "    layer2 = tf.nn.max_pool(conv2, ksize=[1, PF_w, PF_h, 1], strides=[1, PS_w, PS_h, 1], padding='SAME')\n",
    "\n",
    "    shape = layer2.get_shape().as_list()\n",
    "    reshape = tf.reshape(layer2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    FClayer = tf.nn.relu(tf.add(tf.matmul(reshape, model.fc_weights), model.fc_biases))\n",
    "    FClayer = tf.nn.dropout(FClayer,p_keep_conv)\n",
    "    softmax_layer = tf.add(tf.matmul(FClayer, model.Softmax_weights), model.Softmax_biases)\n",
    "    return softmax_layer\n",
    "###############main#####################\n",
    "\n",
    "sess = tf.Session()\n",
    "# the dimension of the network\n",
    "model = dimensions()\n",
    "# loading data \n",
    "data_train, data_test, label_train, label_test =  load_data(model)\n",
    "# splitting the data in to train and test (25:75 as samples from one rpm for train and 3 rpm data for test\n",
    "data_train, data_test, = reshapedata(data_train, data_test, model)\n",
    "# input output placeholders\n",
    "x  = tf.placeholder(tf.float32, [model.BATCH_SIZE, model.input_width,model.input_height,model.input_depth]) # last column = 1 -> channels here is 1 , for RGB = 3\n",
    "y_ = tf.placeholder(tf.float32, [model.BATCH_SIZE, model.No_Classes])\n",
    "# dropout rate\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "# proposed CNN\n",
    "y  = mycnn(x,model, p_keep_conv)\n",
    "# loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "# train step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "# prediction\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "# confusion matrix\n",
    "confusion = tf.confusion_matrix(labels=tf.argmax(y_,1),predictions=tf.argmax(y,1),num_classes=model.No_Classes,dtype=tf.int32)\n",
    "# compute accuracy formula\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "lossfun = np.zeros(model.no_iterations)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# running the training \n",
    "for i in range(model.no_iterations):\n",
    "    image_batch, label_batch = batchdata(data_train, label_train, model.BATCH_SIZE)\n",
    "    epoch_loss = 0\n",
    "    for j in range(model.BATCH_SIZE):\n",
    "        sess.run(train_step, feed_dict={x: image_batch, y_: label_batch, p_keep_conv:model.dropoutrate_train})\n",
    "        c = sess.run( cost, feed_dict={x: image_batch, y_: label_batch, p_keep_conv: model.dropoutrate_train})\n",
    "        epoch_loss += c\n",
    "    lossfun[i] = epoch_loss\n",
    "    print('Epoch',i,'completed out of',model.no_iterations,'loss:',epoch_loss )\n",
    "\n",
    "# testing the training accuracy\n",
    "image_batch, label_batch = batchdata(data_test,label_test,model.BATCH_SIZE)\n",
    "print('train accuracy: ')\n",
    "print((sess.run(accuracy, feed_dict={x: image_batch, y_: label_batch, p_keep_conv:model.dropoutrate_test}))*100)\n",
    "# print the loss curve\n",
    "# we can observe if the network is properly trained if the graph satisfy the P5 plot\n",
    "\n",
    "# running the testing\n",
    "b = np.zeros([1,4])\n",
    "cc = 0\n",
    "# accuracy is computed on k fold cross validataion\n",
    "for i in range(model.cross_validation):\n",
    "    image_batch_test, label_batch_test = batchdata(data_test,label_test,model.BATCH_SIZE)\n",
    "    c = sess.run(confusion, feed_dict={x: image_batch_test, y_: label_batch_test, p_keep_conv:model.dropoutrate_test})\n",
    "    cc += c\n",
    "# final test accuracy of the individual classes\n",
    "print(cc)\n",
    "b[0,0] = (cc[0,0]/np.sum(cc[0]))*100\n",
    "b[0,1]  = (cc[1,1]/np.sum(cc[1]))*100\n",
    "b[0,2]  = (cc[2,2]/np.sum(cc[2]))*100\n",
    "b[0,3]  = (cc[3,3]/np.sum(cc[3]))*100\n",
    "\n",
    "# overall accuracy of the proposed algorithm\n",
    "print('Test Accuracy:')\n",
    "print(b)\n",
    "print(np.mean(b))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
